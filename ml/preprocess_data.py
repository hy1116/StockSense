"""ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ (ML í”¼ì²˜ ìƒì„±)"""
import sys
import os
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Optional

# Windows ì½˜ì†” ì¸ì½”ë”© ì„¤ì •
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))


class DataPreprocessor:
    """ë°ì´í„° ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""

    def __init__(self, data_dir: str = "./data"):
        self.data_dir = Path(data_dir)
        self.raw_dir = self.data_dir / "raw" / "historical"
        self.processed_dir = self.data_dir / "processed"
        self.datasets_dir = self.data_dir / "datasets"

        self.processed_dir.mkdir(parents=True, exist_ok=True)
        self.datasets_dir.mkdir(parents=True, exist_ok=True)

    def load_stock_data(self, stock_code: str) -> pd.DataFrame:
        """CSV íŒŒì¼ì—ì„œ ì£¼ê°€ ë°ì´í„° ë¡œë“œ"""
        filename = self.raw_dir / f"{stock_code}_historical.csv"

        if not filename.exists():
            print(f"âš ï¸  File not found: {filename}")
            return None

        df = pd.read_csv(filename)
        df["date"] = pd.to_datetime(df["date"])
        df = df.sort_values("date").reset_index(drop=True)

        return df

    def create_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê¸°ìˆ ì  ì§€í‘œ ìƒì„±"""
        df = df.copy()

        # ì´ë™í‰ê·  (30ì¼ ë°ì´í„°ë¥¼ ê³ ë ¤í•˜ì—¬ ìµœëŒ€ 20ì¼ë¡œ ì œí•œ)
        df["ma5"] = df["close"].rolling(window=5).mean()
        df["ma10"] = df["close"].rolling(window=10).mean()
        df["ma20"] = df["close"].rolling(window=20).mean()

        # RSI (Relative Strength Index)
        delta = df["close"].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df["rsi"] = 100 - (100 / (1 + rs))

        # ë³¼ë¦°ì € ë°´ë“œ
        df["bb_middle"] = df["close"].rolling(window=20).mean()
        df["bb_std"] = df["close"].rolling(window=20).std()
        df["bb_upper"] = df["bb_middle"] + (df["bb_std"] * 2)
        df["bb_lower"] = df["bb_middle"] - (df["bb_std"] * 2)
        df["bb_width"] = (df["bb_upper"] - df["bb_lower"]) / df["bb_middle"]

        # MACD
        ema_12 = df["close"].ewm(span=12, adjust=False).mean()
        ema_26 = df["close"].ewm(span=26, adjust=False).mean()
        df["macd"] = ema_12 - ema_26
        df["macd_signal"] = df["macd"].ewm(span=9, adjust=False).mean()
        df["macd_diff"] = df["macd"] - df["macd_signal"]

        # ê±°ë˜ëŸ‰ ì´ë™í‰ê· 
        df["volume_ma5"] = df["volume"].rolling(window=5).mean()
        df["volume_ma20"] = df["volume"].rolling(window=20).mean()

        # ê°€ê²© ë³€í™”ìœ¨
        df["price_change_1d"] = df["close"].pct_change(1)
        df["price_change_5d"] = df["close"].pct_change(5)
        df["price_change_20d"] = df["close"].pct_change(20)

        # ê±°ë˜ëŸ‰ ë³€í™”ìœ¨
        df["volume_change"] = df["volume"].pct_change(1)

        # ë³€ë™ì„± (í‘œì¤€í¸ì°¨)
        df["volatility_5d"] = df["close"].rolling(window=5).std()
        df["volatility_20d"] = df["close"].rolling(window=20).std()

        # ê³ ê°€/ì €ê°€ ë¹„ìœ¨
        df["high_low_ratio"] = df["high"] / df["low"]

        # ì¢…ê°€/ì‹œê°€ ë¹„ìœ¨
        df["close_open_ratio"] = df["close"] / df["open"]

        return df

    def create_target(self, df: pd.DataFrame, days_ahead: int = 1) -> pd.DataFrame:
        """íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ë¯¸ë˜ ê°€ê²©)"""
        df = df.copy()

        # ë‹¤ìŒë‚  ì¢…ê°€
        df["target_price"] = df["close"].shift(-days_ahead)

        # ë‹¤ìŒë‚  ìˆ˜ìµë¥ 
        df["target_return"] = (df["target_price"] / df["close"]) - 1

        # ìƒìŠ¹/í•˜ë½ (ë¶„ë¥˜ìš©)
        df["target_direction"] = (df["target_return"] > 0).astype(int)

        return df

    def load_news_sentiment(self, stock_code: str) -> Optional[pd.DataFrame]:
        """DBì—ì„œ ì¢…ëª©ë³„ ë‰´ìŠ¤ ê°ì„± ë°ì´í„°ë¥¼ ì¼ë³„ë¡œ ì§‘ê³„

        Returns:
            ë‚ ì§œë³„ ë‰´ìŠ¤ ê°ì„± í”¼ì²˜ DataFrame:
            - news_sentiment_avg: ë‹¹ì¼ í‰ê·  ê°ì„± ì ìˆ˜ (-100~100)
            - news_count: ë‹¹ì¼ ë‰´ìŠ¤ ê±´ìˆ˜
            - news_positive_ratio: ê¸ì • ë‰´ìŠ¤ ë¹„ìœ¨ (0~1)
            - news_negative_ratio: ë¶€ì • ë‰´ìŠ¤ ë¹„ìœ¨ (0~1)
        """
        try:
            from dotenv import load_dotenv
            from sqlalchemy import create_engine, text

            load_dotenv()
            db_url = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/stocksense")
            db_url = db_url.replace("+asyncpg", "")
            engine = create_engine(db_url)

            query = text("""
                SELECT
                    DATE(published_at) as date,
                    AVG(sentiment_score) as news_sentiment_avg,
                    COUNT(*) as news_count,
                    SUM(CASE WHEN sentiment_label = 'positive' THEN 1 ELSE 0 END)::float
                        / COUNT(*) as news_positive_ratio,
                    SUM(CASE WHEN sentiment_label = 'negative' THEN 1 ELSE 0 END)::float
                        / COUNT(*) as news_negative_ratio
                FROM stock_news
                WHERE stock_code = :stock_code
                    AND is_processed = true
                    AND published_at IS NOT NULL
                    AND sentiment_score IS NOT NULL
                GROUP BY DATE(published_at)
                ORDER BY date
            """)

            df = pd.read_sql(query, engine, params={"stock_code": stock_code})

            if df.empty:
                print(f"   No news sentiment data for {stock_code}")
                return None

            df["date"] = pd.to_datetime(df["date"])
            print(f"   Loaded {len(df)} days of news sentiment data")
            return df

        except Exception as e:
            print(f"   âš ï¸ Failed to load news sentiment: {e}")
            return None

    def merge_news_features(self, df: pd.DataFrame, stock_code: str) -> pd.DataFrame:
        """ë‰´ìŠ¤ ê°ì„± í”¼ì²˜ë¥¼ ê¸°ìˆ ì  ì§€í‘œ DataFrameì— ë³‘í•©"""
        news_df = self.load_news_sentiment(stock_code)

        # ë‰´ìŠ¤ í”¼ì²˜ ì»¬ëŸ¼ ì´ˆê¸°í™” (ë‰´ìŠ¤ ë°ì´í„° ì—†ì–´ë„ ì»¬ëŸ¼ ì¡´ì¬í•´ì•¼ í•¨)
        df["news_sentiment_avg"] = 0.0
        df["news_count"] = 0
        df["news_positive_ratio"] = 0.0
        df["news_negative_ratio"] = 0.0

        if news_df is not None and not news_df.empty:
            # ë‚ ì§œ ê¸°ì¤€ LEFT JOIN
            df = df.merge(
                news_df[["date", "news_sentiment_avg", "news_count",
                         "news_positive_ratio", "news_negative_ratio"]],
                on="date",
                how="left",
                suffixes=("_drop", "")
            )

            # ë³‘í•©ìœ¼ë¡œ ì¸í•œ ì¤‘ë³µ ì»¬ëŸ¼ ì²˜ë¦¬
            drop_cols = [c for c in df.columns if c.endswith("_drop")]
            if drop_cols:
                df = df.drop(columns=drop_cols)

            # ë‰´ìŠ¤ ì—†ëŠ” ë‚  0ìœ¼ë¡œ ì±„ì›€
            news_cols = ["news_sentiment_avg", "news_count",
                        "news_positive_ratio", "news_negative_ratio"]
            df[news_cols] = df[news_cols].fillna(0)

            merged_count = (df["news_count"] > 0).sum()
            print(f"   Merged news features: {merged_count} days with news data")

        return df

    def preprocess_stock(self, stock_code: str, save: bool = True) -> pd.DataFrame:
        """íŠ¹ì • ì¢…ëª© ì „ì²˜ë¦¬"""
        print(f"\nğŸ“Š Preprocessing {stock_code}...")

        # 1. ë°ì´í„° ë¡œë“œ
        df = self.load_stock_data(stock_code)
        if df is None:
            return None

        print(f"   Loaded {len(df)} records")

        # 2. ê¸°ìˆ ì  ì§€í‘œ ìƒì„±
        df = self.create_technical_indicators(df)
        print(f"   Created technical indicators")

        # 3. ë‰´ìŠ¤ ê°ì„± í”¼ì²˜ ë³‘í•©
        df = self.merge_news_features(df, stock_code)
        print(f"   Merged news sentiment features")

        # 4. íƒ€ê²Ÿ ìƒì„±
        df = self.create_target(df, days_ahead=1)
        print(f"   Created target variables")

        # 5. NaN ì œê±°
        df_clean = df.dropna()
        print(f"   Cleaned data: {len(df_clean)} records (removed {len(df) - len(df_clean)} NaN rows)")

        # 6. ì €ì¥
        if save:
            output_file = self.processed_dir / f"{stock_code}_features.csv"
            df_clean.to_csv(output_file, index=False, encoding="utf-8-sig")
            print(f"   ğŸ’¾ Saved to {output_file.name}")

        return df_clean

    def combine_all_stocks(self) -> pd.DataFrame:
        """ëª¨ë“  ì¢…ëª© ë°ì´í„° ê²°í•©"""
        print("\nğŸ”— Combining all stock data...")

        all_files = list(self.processed_dir.glob("*_features.csv"))

        if not all_files:
            print("âš ï¸  No processed files found. Run preprocessing first.")
            return None

        dfs = []
        for file in all_files:
            df = pd.read_csv(file)
            dfs.append(df)
            print(f"   Loaded {file.name}: {len(df)} records")

        combined = pd.concat(dfs, ignore_index=True)
        print(f"\nâœ… Combined {len(all_files)} files: Total {len(combined)} records")

        return combined

    def split_dataset(
        self, df: pd.DataFrame, train_ratio: float = 0.7, val_ratio: float = 0.15
    ):
        """í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• """
        print("\nâœ‚ï¸  Splitting dataset...")

        # ì‹œê³„ì—´ ë°ì´í„°ëŠ” ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë¶„í•  (shuffle X)
        n = len(df)
        train_end = int(n * train_ratio)
        val_end = int(n * (train_ratio + val_ratio))

        train_df = df.iloc[:train_end]
        val_df = df.iloc[train_end:val_end]
        test_df = df.iloc[val_end:]

        print(f"   Train: {len(train_df)} records ({train_ratio*100:.0f}%)")
        print(f"   Validation: {len(val_df)} records ({val_ratio*100:.0f}%)")
        print(f"   Test: {len(test_df)} records ({(1-train_ratio-val_ratio)*100:.0f}%)")

        # ì €ì¥
        train_df.to_csv(self.datasets_dir / "train.csv", index=False, encoding="utf-8-sig")
        val_df.to_csv(self.datasets_dir / "validation.csv", index=False, encoding="utf-8-sig")
        test_df.to_csv(self.datasets_dir / "test.csv", index=False, encoding="utf-8-sig")

        print(f"   ğŸ’¾ Saved to {self.datasets_dir}")

        return train_df, val_df, test_df

    def process_all(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("\n" + "=" * 50)
        print("ğŸš€ Starting data preprocessing pipeline")
        print("=" * 50)

        # 1. ëª¨ë“  ì¢…ëª© ì „ì²˜ë¦¬
        raw_files = list(self.raw_dir.glob("*_historical.csv"))

        if not raw_files:
            print("âš ï¸  No raw data files found.")
            print("   Please run: python scripts/collect_historical_data.py")
            return

        print(f"\nFound {len(raw_files)} stock files to process\n")

        for file in raw_files:
            stock_code = file.stem.replace("_historical", "")
            self.preprocess_stock(stock_code, save=True)

        # 2. ì „ì²´ ë°ì´í„° ê²°í•©
        combined_df = self.combine_all_stocks()

        if combined_df is None:
            return

        # 3. í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• 
        self.split_dataset(combined_df)

        print("\n" + "=" * 50)
        print("âœ… Data preprocessing completed!")
        print("=" * 50)
        print(f"\nğŸ“ Processed files: {self.processed_dir}")
        print(f"ğŸ“ Dataset files: {self.datasets_dir}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="ë°ì´í„° ì „ì²˜ë¦¬ ë° í”¼ì²˜ ìƒì„±")
    parser.add_argument("--stock", type=str, help="íŠ¹ì • ì¢…ëª©ë§Œ ì²˜ë¦¬ (ì˜ˆ: 005930)")

    args = parser.parse_args()

    preprocessor = DataPreprocessor()

    if args.stock:
        # íŠ¹ì • ì¢…ëª©ë§Œ ì²˜ë¦¬
        preprocessor.preprocess_stock(args.stock, save=True)
    else:
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        preprocessor.process_all()


if __name__ == "__main__":
    main()
