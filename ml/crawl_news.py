"""ë‰´ìŠ¤ í¬ë¡¤ë§ ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸

1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰í•˜ì—¬ ì¢…ëª©ë³„ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

Usage:
    # ê¸°ë³¸ ì‹¤í–‰ (ê´€ì‹¬ ì¢…ëª©, ìµœê·¼ 1ì‹œê°„ ë‰´ìŠ¤ë§Œ)
    python -m ml.crawl_news

    # ëª¨ë“  ìˆ˜ì§‘ ëŒ€ìƒ ì¢…ëª©
    python -m ml.crawl_news --all

    # íŠ¹ì • ì¢…ëª©ë§Œ
    python -m ml.crawl_news --stock-code 005930 --stock-name ì‚¼ì„±ì „ì

    # ìµœê·¼ Nì‹œê°„ ë‰´ìŠ¤
    python -m ml.crawl_news --hours 2

    # ìµœê·¼ Nì¼ ë‰´ìŠ¤ (ë„¤ì´ë²„ ê²€ìƒ‰ ì‚¬ìš©)
    python -m ml.crawl_news --days 7 --use-search
"""
import sys
import os
import asyncio
import argparse
from pathlib import Path
from datetime import datetime, timedelta, timezone

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Windows ì½˜ì†” ì¸ì½”ë”©
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')


async def crawl_news_for_stocks(
    stocks: list,
    max_pages: int = 3,
    use_search: bool = False,
    days: int = 7,
    hours: int = None
):
    """ì—¬ëŸ¬ ì¢…ëª©ì˜ ë‰´ìŠ¤ í¬ë¡¤ë§

    Args:
        stocks: ì¢…ëª© ë¦¬ìŠ¤íŠ¸ [{'code': '005930', 'name': 'ì‚¼ì„±ì „ì'}, ...]
        max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        use_search: ë„¤ì´ë²„ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
        days: ìµœê·¼ Nì¼ (use_search=Trueì¼ ë•Œ ì‚¬ìš©)
        hours: ìµœê·¼ Nì‹œê°„ ë‚´ ë‰´ìŠ¤ë§Œ í•„í„°ë§ (Noneì´ë©´ í•„í„°ë§ ì•ˆí•¨)
    """
    from dotenv import load_dotenv
    from sqlalchemy import create_engine, select
    from sqlalchemy.orm import sessionmaker

    from app.models.stock_news import StockNews
    from app.services.news_crawler import crawl_stock_news, search_stock_news

    load_dotenv()

    # DB ì—°ê²°
    db_url = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/stocksense")
    db_url = db_url.replace("+asyncpg", "")
    engine = create_engine(db_url)
    Session = sessionmaker(bind=engine)

    # ì‹œê°„ í•„í„° ê¸°ì¤€
    time_threshold = None
    if hours:
        KST = timezone(timedelta(hours=9))
        time_threshold = datetime.now(KST) - timedelta(hours=hours)

    total_crawled = 0
    total_saved = 0
    total_duplicate = 0
    total_filtered = 0

    print(f"\n{'='*60}")
    print(f"ğŸ“° News Crawling Batch Started")
    print(f"   Stocks: {len(stocks)}")
    print(f"   Method: {'Search' if use_search else 'Finance News'}")
    print(f"   Max Pages: {max_pages}")
    if hours:
        print(f"   Time Filter: Last {hours} hour(s)")
    print(f"{'='*60}\n")

    for stock in stocks:
        stock_code = stock['code']
        stock_name = stock['name']

        print(f"\nğŸ” Crawling news for {stock_name} ({stock_code})...")

        try:
            # í¬ë¡¤ë§ ë°©ì‹ ì„ íƒ
            if use_search:
                news_list = await search_stock_news(
                    stock_name=stock_name,
                    stock_code=stock_code,
                    days=days
                )
            else:
                news_list = await crawl_stock_news(
                    stock_code=stock_code,
                    stock_name=stock_name,
                    max_pages=max_pages
                )

            crawled_count = len(news_list)
            saved_count = 0
            duplicate_count = 0
            filtered_count = 0

            # DBì— ì €ì¥
            with Session() as session:
                for news_item in news_list:
                    try:
                        # ì‹œê°„ í•„í„°ë§ (hours ì˜µì…˜ì´ ìˆì„ ë•Œ)
                        if time_threshold and news_item.get('published_at'):
                            if news_item['published_at'] < time_threshold:
                                filtered_count += 1
                                continue

                        # ì¤‘ë³µ ì²´í¬
                        existing = session.execute(
                            select(StockNews).where(StockNews.url == news_item['url'])
                        ).scalar_one_or_none()

                        if existing:
                            duplicate_count += 1
                            continue

                        # ìƒˆ ë‰´ìŠ¤ ì €ì¥
                        news = StockNews(
                            stock_code=news_item['stock_code'],
                            stock_name=news_item.get('stock_name'),
                            title=news_item['title'],
                            url=news_item['url'],
                            source=news_item.get('source'),
                            content=news_item.get('content'),
                            image_url=news_item.get('image_url'),
                            published_at=news_item.get('published_at'),
                        )
                        session.add(news)
                        saved_count += 1

                    except Exception as e:
                        print(f"   âš ï¸ Error saving: {e}")
                        continue

                session.commit()

            total_crawled += crawled_count
            total_saved += saved_count
            total_duplicate += duplicate_count
            total_filtered += filtered_count

            print(f"   âœ… Crawled: {crawled_count}, Saved: {saved_count}, Duplicates: {duplicate_count}, Filtered(old): {filtered_count}")

            # ìš”ì²­ ê°„ ë”œë ˆì´
            await asyncio.sleep(1)

        except Exception as e:
            print(f"   âŒ Error: {e}")
            continue

    print(f"\n{'='*60}")
    print(f"ğŸ“Š Crawling Summary")
    print(f"   Total Crawled: {total_crawled}")
    print(f"   Total Saved: {total_saved}")
    print(f"   Total Duplicates: {total_duplicate}")
    print(f"   Total Filtered (old): {total_filtered}")
    print(f"{'='*60}\n")

    return {
        'total_crawled': total_crawled,
        'total_saved': total_saved,
        'total_duplicate': total_duplicate,
        'total_filtered': total_filtered
    }


def get_collection_stocks():
    """ìˆ˜ì§‘ ëŒ€ìƒ ì¢…ëª© ëª©ë¡ ì¡°íšŒ"""
    from dotenv import load_dotenv
    from sqlalchemy import create_engine, select, text
    from sqlalchemy.orm import sessionmaker

    load_dotenv()

    db_url = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/stocksense")
    db_url = db_url.replace("+asyncpg", "")
    engine = create_engine(db_url)

    stocks = []

    with engine.connect() as conn:
        # collection_stocks í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
        result = conn.execute(text("""
            SELECT stock_code, stock_name
            FROM stocks
            WHERE is_active = true
            ORDER BY stock_name
        """))

        for row in result:
            stocks.append({
                'code': row[0],
                'name': row[1]
            })

    return stocks


def get_default_stocks():
    """ê¸°ë³¸ ì£¼ìš” ì¢…ëª© ëª©ë¡"""
    return [
        {'code': '005930', 'name': 'ì‚¼ì„±ì „ì'},
        {'code': '000660', 'name': 'SKí•˜ì´ë‹‰ìŠ¤'},
        {'code': '035420', 'name': 'NAVER'},
        {'code': '035720', 'name': 'ì¹´ì¹´ì˜¤'},
        {'code': '005380', 'name': 'í˜„ëŒ€ì°¨'},
        {'code': '006400', 'name': 'ì‚¼ì„±SDI'},
        {'code': '051910', 'name': 'LGí™”í•™'},
        {'code': '003670', 'name': 'í¬ìŠ¤ì½”í“¨ì²˜ì— '},
        {'code': '005490', 'name': 'POSCOí™€ë”©ìŠ¤'},
        {'code': '055550', 'name': 'ì‹ í•œì§€ì£¼'},
    ]


async def main():
    parser = argparse.ArgumentParser(description='Stock News Crawler')
    parser.add_argument('--all', action='store_true',
                        help='Crawl all collection stocks')
    parser.add_argument('--stock-code', '-c', type=str,
                        help='Specific stock code')
    parser.add_argument('--stock-name', '-n', type=str,
                        help='Specific stock name')
    parser.add_argument('--max-pages', '-p', type=int, default=2,
                        help='Max pages to crawl (default: 2)')
    parser.add_argument('--use-search', '-s', action='store_true',
                        help='Use Naver search instead of finance news')
    parser.add_argument('--days', '-d', type=int, default=7,
                        help='Days to search (only with --use-search)')
    parser.add_argument('--hours', '-H', type=int, default=1,
                        help='Filter news within last N hours (default: 1, 0=no filter)')

    args = parser.parse_args()

    # ì¢…ëª© ëª©ë¡ ê²°ì •
    if args.stock_code:
        if not args.stock_name:
            print("Error: --stock-name is required with --stock-code")
            return
        stocks = [{'code': args.stock_code, 'name': args.stock_name}]
    elif args.all:
        print("ğŸ“‹ Loading collection stocks from DB...")
        stocks = get_collection_stocks()
        if not stocks:
            print("âš ï¸ No collection stocks found, using defaults")
            stocks = get_default_stocks()
    else:
        stocks = get_default_stocks()

    print(f"ğŸ“Œ Target stocks: {len(stocks)}")
    for s in stocks[:5]:
        print(f"   - {s['name']} ({s['code']})")
    if len(stocks) > 5:
        print(f"   ... and {len(stocks) - 5} more")

    # ì‹œê°„ í•„í„° (0ì´ë©´ í•„í„°ë§ ì•ˆí•¨)
    hours_filter = args.hours if args.hours > 0 else None

    # í¬ë¡¤ë§ ì‹¤í–‰
    result = await crawl_news_for_stocks(
        stocks=stocks,
        max_pages=args.max_pages,
        use_search=args.use_search,
        days=args.days,
        hours=hours_filter
    )

    print(f"\nâœ… News crawling completed at {datetime.now().isoformat()}")


if __name__ == '__main__':
    asyncio.run(main())
