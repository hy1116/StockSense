"""ë‰´ìŠ¤ í¬ë¡¤ë§ ì„œë¹„ìŠ¤"""
import logging
import re
import asyncio
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional
from urllib.parse import quote, urljoin

KST = timezone(timedelta(hours=9))

import httpx
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


class NewsCrawler:
    """ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""

    # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ URL
    NAVER_FINANCE_NEWS_URL = "https://finance.naver.com/item/news_news.naver"
    NAVER_NEWS_DETAIL_URL = "https://finance.naver.com"

    # ìš”ì²­ í—¤ë”
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': 'https://finance.naver.com/',
    }

    def __init__(self):
        self.client = None

    async def __aenter__(self):
        self.client = httpx.AsyncClient(headers=self.HEADERS, timeout=30.0)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.client:
            await self.client.aclose()

    async def crawl_stock_news(
        self,
        stock_code: str,
        stock_name: str = None,
        page: int = 1,
        max_pages: int = 3
    ) -> List[Dict]:
        """ì¢…ëª©ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§

        Args:
            stock_code: ì¢…ëª©ì½”ë“œ (ì˜ˆ: 005930)
            stock_name: ì¢…ëª©ëª… (ì˜ˆ: ì‚¼ì„±ì „ì)
            page: ì‹œì‘ í˜ì´ì§€
            max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜

        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        all_news = []

        for p in range(page, page + max_pages):
            try:
                news_list = await self._crawl_news_page(stock_code, stock_name, p)
                if not news_list:
                    break  # ë” ì´ìƒ ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨

                all_news.extend(news_list)
                logger.info(f"ğŸ“° Crawled page {p} for {stock_code}: {len(news_list)} news")

                # ìš”ì²­ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                await asyncio.sleep(0.5)

            except Exception as e:
                logger.error(f"âŒ Error crawling page {p} for {stock_code}: {e}")
                break

        return all_news

    async def _crawl_news_page(
        self,
        stock_code: str,
        stock_name: str,
        page: int
    ) -> List[Dict]:
        """ë‹¨ì¼ í˜ì´ì§€ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        params = {
            'code': stock_code,
            'page': page,
            'sm': 'title_entity_id.basic',
            'clusterId': ''
        }

        try:
            response = await self.client.get(self.NAVER_FINANCE_NEWS_URL, params=params)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            news_list = []

            # ë‰´ìŠ¤ í…Œì´ë¸”ì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ
            news_table = soup.select('table.type5 tbody tr')

            for row in news_table:
                try:
                    # ì œëª©ì´ ìˆëŠ” í–‰ë§Œ ì²˜ë¦¬
                    title_elem = row.select_one('td.title a')
                    if not title_elem:
                        continue

                    title = title_elem.get_text(strip=True)
                    url = title_elem.get('href', '')

                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if url and not url.startswith('http'):
                        url = urljoin(self.NAVER_NEWS_DETAIL_URL, url)

                    # ì¶œì²˜
                    source_elem = row.select_one('td.info')
                    source = source_elem.get_text(strip=True) if source_elem else None

                    # ë‚ ì§œ
                    date_elem = row.select_one('td.date')
                    date_str = date_elem.get_text(strip=True) if date_elem else None
                    published_at = self._parse_date(date_str)

                    news_item = {
                        'stock_code': stock_code,
                        'stock_name': stock_name,
                        'title': title,
                        'url': url,
                        'source': source,
                        'published_at': published_at,
                        'content': None,  # ìƒì„¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
                        'image_url': None,
                    }

                    news_list.append(news_item)

                except Exception as e:
                    logger.warning(f"âš ï¸ Error parsing news row: {e}")
                    continue

            return news_list

        except httpx.HTTPError as e:
            logger.error(f"âŒ HTTP error: {e}")
            return []

    async def crawl_news_content(self, url: str) -> Optional[Dict]:
        """ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§

        Args:
            url: ë‰´ìŠ¤ URL

        Returns:
            ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© ë”•ì…”ë„ˆë¦¬
        """
        try:
            response = await self.client.get(url)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # ë³¸ë¬¸ ì¶”ì¶œ (ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ êµ¬ì¡°)
            content = None
            content_elem = soup.select_one('div#news_read')
            if content_elem:
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for tag in content_elem.select('script, style, iframe'):
                    tag.decompose()
                content = content_elem.get_text(strip=True)

            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            image_url = None
            img_elem = soup.select_one('div#news_read img')
            if img_elem:
                image_url = img_elem.get('src')

            # ê¸°ìëª… ì¶”ì¶œ
            author = None
            author_elem = soup.select_one('span.byline')
            if author_elem:
                author = author_elem.get_text(strip=True)

            return {
                'content': content,
                'image_url': image_url,
                'author': author
            }

        except Exception as e:
            logger.warning(f"âš ï¸ Error crawling news content: {e}")
            return None

    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹± (KST timezone-aware)"""
        if not date_str:
            return None

        now = datetime.now(KST)

        try:
            # "2024.01.15 10:30" í˜•ì‹
            if '.' in date_str:
                naive = datetime.strptime(date_str, "%Y.%m.%d %H:%M")
                return naive.replace(tzinfo=KST)

            # "1ì‹œê°„ ì „", "2ì¼ ì „" ë“±
            if 'ë¶„ ì „' in date_str:
                minutes = int(re.search(r'(\d+)', date_str).group(1))
                return now - timedelta(minutes=minutes)

            if 'ì‹œê°„ ì „' in date_str:
                hours = int(re.search(r'(\d+)', date_str).group(1))
                return now - timedelta(hours=hours)

            if 'ì¼ ì „' in date_str:
                days = int(re.search(r'(\d+)', date_str).group(1))
                return now - timedelta(days=days)

            return None

        except Exception:
            return None


class NaverSearchNewsCrawler:
    """ë„¤ì´ë²„ ê²€ìƒ‰ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ë” ë§ì€ ë‰´ìŠ¤ ìˆ˜ì§‘ìš©)"""

    NAVER_SEARCH_URL = "https://search.naver.com/search.naver"

    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    }

    def __init__(self):
        self.client = None

    async def __aenter__(self):
        self.client = httpx.AsyncClient(headers=self.HEADERS, timeout=30.0)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.client:
            await self.client.aclose()

    async def search_news(
        self,
        stock_name: str,
        stock_code: str,
        days: int = 7,
        max_results: int = 50
    ) -> List[Dict]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰

        Args:
            stock_name: ì¢…ëª©ëª…
            stock_code: ì¢…ëª©ì½”ë“œ
            days: ìµœê·¼ Nì¼ ì´ë‚´ ë‰´ìŠ¤
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        all_news = []
        start = 1

        while len(all_news) < max_results:
            try:
                params = {
                    'where': 'news',
                    'query': f'{stock_name} ì£¼ì‹',
                    'sort': 1,  # ìµœì‹ ìˆœ
                    'start': start,
                    'nso': f'so:dd,p:from{self._get_date_str(days)},a:all'
                }

                response = await self.client.get(self.NAVER_SEARCH_URL, params=params)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser')
                news_items = soup.select('div.news_area')

                if not news_items:
                    break

                for item in news_items:
                    try:
                        # ì œëª©
                        title_elem = item.select_one('a.news_tit')
                        if not title_elem:
                            continue

                        title = title_elem.get_text(strip=True)
                        url = title_elem.get('href', '')

                        # ì¶œì²˜
                        source_elem = item.select_one('a.info.press')
                        source = source_elem.get_text(strip=True) if source_elem else None

                        # ìš”ì•½
                        desc_elem = item.select_one('div.news_dsc')
                        content = desc_elem.get_text(strip=True) if desc_elem else None

                        # ë‚ ì§œ
                        date_elem = item.select_one('span.info')
                        date_str = date_elem.get_text(strip=True) if date_elem else None
                        published_at = self._parse_relative_date(date_str)

                        # ì´ë¯¸ì§€
                        img_elem = item.select_one('img.thumb')
                        image_url = img_elem.get('src') if img_elem else None

                        news_item = {
                            'stock_code': stock_code,
                            'stock_name': stock_name,
                            'title': title,
                            'url': url,
                            'source': source,
                            'content': content,
                            'published_at': published_at,
                            'image_url': image_url,
                        }

                        all_news.append(news_item)

                    except Exception as e:
                        logger.warning(f"âš ï¸ Error parsing search result: {e}")
                        continue

                start += 10
                await asyncio.sleep(0.5)

            except Exception as e:
                logger.error(f"âŒ Error searching news: {e}")
                break

        return all_news[:max_results]

    def _get_date_str(self, days: int) -> str:
        """ê²€ìƒ‰ ë‚ ì§œ ë¬¸ìì—´ ìƒì„±"""
        date = datetime.now(KST) - timedelta(days=days)
        return date.strftime("%Y%m%d")

    def _parse_relative_date(self, date_str: str) -> Optional[datetime]:
        """ìƒëŒ€ ë‚ ì§œ íŒŒì‹± (KST timezone-aware)"""
        if not date_str:
            return None

        now = datetime.now(KST)

        try:
            if 'ë¶„ ì „' in date_str:
                minutes = int(re.search(r'(\d+)', date_str).group(1))
                return now - timedelta(minutes=minutes)

            if 'ì‹œê°„ ì „' in date_str:
                hours = int(re.search(r'(\d+)', date_str).group(1))
                return now - timedelta(hours=hours)

            if 'ì¼ ì „' in date_str:
                days = int(re.search(r'(\d+)', date_str).group(1))
                return now - timedelta(days=days)

            # "2024.01.15." í˜•ì‹
            match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})', date_str)
            if match:
                return datetime(int(match.group(1)), int(match.group(2)), int(match.group(3)), tzinfo=KST)

            return None

        except Exception:
            return None


# í¸ì˜ í•¨ìˆ˜
async def crawl_stock_news(stock_code: str, stock_name: str = None, max_pages: int = 3) -> List[Dict]:
    """ì¢…ëª© ë‰´ìŠ¤ í¬ë¡¤ë§ í—¬í¼ í•¨ìˆ˜"""
    async with NewsCrawler() as crawler:
        return await crawler.crawl_stock_news(stock_code, stock_name, max_pages=max_pages)


async def search_stock_news(stock_name: str, stock_code: str, days: int = 7) -> List[Dict]:
    """ë„¤ì´ë²„ ê²€ìƒ‰ ë‰´ìŠ¤ í—¬í¼ í•¨ìˆ˜"""
    async with NaverSearchNewsCrawler() as crawler:
        return await crawler.search_news(stock_name, stock_code, days=days)
